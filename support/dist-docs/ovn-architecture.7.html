<html><head><meta charset="UTF-8"></head><body><pre>
ovn-architecture(7)           Open vSwitch Manual          ovn-architecture(7)



<b>N</b><b>A</b><b>M</b><b>E</b>
       ovn-architecture - Open Virtual Network architecture

<b>D</b><b>E</b><b>S</b><b>C</b><b>R</b><b>I</b><b>P</b><b>T</b><b>I</b><b>O</b><b>N</b>
       OVN,  the  Open Virtual Network, is a system to support virtual network
       abstraction.  OVN complements the existing capabilities of OVS  to  add
       native support for virtual network abstractions, such as virtual L2 and
       L3 overlays and security groups.  Services such as DHCP are also desir‐
       able  features.   Just like OVS, OVN’s design goal is to have a produc‐
       tion-quality implementation that can operate at significant scale.

       An OVN deployment consists of several components:

              ·      A <u>C</u><u>l</u><u>o</u><u>u</u><u>d</u> <u>M</u><u>a</u><u>n</u><u>a</u><u>g</u><u>e</u><u>m</u><u>e</u><u>n</u><u>t</u> <u>S</u><u>y</u><u>s</u><u>t</u><u>e</u><u>m</u> (<u>C</u><u>M</u><u>S</u>), which is OVN’s  ultimate
                     client  (via its users and administrators).  OVN integra‐
                     tion  requires  installing  a  CMS-specific  plugin   and
                     related  software  (see  below).   OVN  initially targets
                     OpenStack as CMS.

                     We generally speak of ``the’’ CMS, but  one  can  imagine
                     scenarios  in which multiple CMSes manage different parts
                     of an OVN deployment.

              ·      An OVN Database physical or virtual node (or, eventually,
                     cluster) installed in a central location.

              ·      One or more (usually many) <u>h</u><u>y</u><u>p</u><u>e</u><u>r</u><u>v</u><u>i</u><u>s</u><u>o</u><u>r</u><u>s</u>.  Hypervisors must
                     run Open vSwitch and implement the interface described in
                     <b>I</b><b>n</b><b>t</b><b>e</b><b>g</b><b>r</b><b>a</b><b>t</b><b>i</b><b>o</b><b>n</b><b>G</b><b>u</b><b>i</b><b>d</b><b>e</b><b>.</b><b>m</b><b>d</b> in the OVS source tree.  Any hypervi‐
                     sor platform supported by Open vSwitch is acceptable.

              ·      Zero or more <u>g</u><u>a</u><u>t</u><u>e</u><u>w</u><u>a</u><u>y</u><u>s</u>.  A gateway extends a  tunnel-based
                     logical  network  into a physical network by bidirection‐
                     ally forwarding packets between tunnels  and  a  physical
                     Ethernet  port.   This allows non-virtualized machines to
                     participate in logical networks.   A  gateway  may  be  a
                     physical  host, a virtual machine, or an ASIC-based hard‐
                     ware switch that supports the <b>v</b><b>t</b><b>e</b><b>p</b>(5)  schema.   (Support
                     for the latter will come later in OVN implementation.)

                     Hypervisors  and  gateways  are together called <u>t</u><u>r</u><u>a</u><u>n</u><u>s</u><u>p</u><u>o</u><u>r</u><u>t</u>
                     <u>n</u><u>o</u><u>d</u><u>e</u> or <u>c</u><u>h</u><u>a</u><u>s</u><u>s</u><u>i</u><u>s</u>.

       The diagram below shows how the major components  of  OVN  and  related
       software interact.  Starting at the top of the diagram, we have:

              ·      The Cloud Management System, as defined above.

              ·      The  <u>O</u><u>V</u><u>N</u><u>/</u><u>C</u><u>M</u><u>S</u>  <u>P</u><u>l</u><u>u</u><u>g</u><u>i</u><u>n</u>  is  the  component  of the CMS that
                     interfaces to OVN.  In OpenStack, this is a Neutron plug‐
                     in.   The plugin’s main purpose is to translate the CMS’s
                     notion of logical network configuration,  stored  in  the
                     CMS’s  configuration  database  in a CMS-specific format,
                     into an intermediate representation understood by OVN.

                     This component is  necessarily  CMS-specific,  so  a  new
                     plugin  needs  to be developed for each CMS that is inte‐
                     grated with OVN.  All of the components below this one in
                     the diagram are CMS-independent.

              ·      The  <u>O</u><u>V</u><u>N</u>  <u>N</u><u>o</u><u>r</u><u>t</u><u>h</u><u>b</u><u>o</u><u>u</u><u>n</u><u>d</u>  <u>D</u><u>a</u><u>t</u><u>a</u><u>b</u><u>a</u><u>s</u><u>e</u>  receives the intermediate
                     representation of logical  network  configuration  passed
                     down by the OVN/CMS Plugin.  The database schema is meant
                     to be ``impedance matched’’ with the concepts used  in  a
                     CMS,  so  that  it  directly  supports notions of logical
                     switches, routers, ACLs, and so on.   See  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>b</b>(5)  for
                     details.

                     The  OVN  Northbound  Database  has only two clients: the
                     OVN/CMS Plugin above it and <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b> below it.

              ·      <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>(8) connects to  the  OVN  Northbound  Database
                     above  it  and  the OVN Southbound Database below it.  It
                     translates the logical network configuration in terms  of
                     conventional  network concepts, taken from the OVN North‐
                     bound Database, into logical datapath flows  in  the  OVN
                     Southbound Database below it.

              ·      The  <u>O</u><u>V</u><u>N</u> <u>S</u><u>o</u><u>u</u><u>t</u><u>h</u><u>b</u><u>o</u><u>u</u><u>n</u><u>d</u> <u>D</u><u>a</u><u>t</u><u>a</u><u>b</u><u>a</u><u>s</u><u>e</u> is the center of the system.
                     Its clients  are  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>(8)  above  it  and  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>‐</b>
                     <b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>(8) on every transport node below it.

                     The OVN Southbound Database contains three kinds of data:
                     <u>P</u><u>h</u><u>y</u><u>s</u><u>i</u><u>c</u><u>a</u><u>l</u> <u>N</u><u>e</u><u>t</u><u>w</u><u>o</u><u>r</u><u>k</u> (PN) tables that specify  how  to  reach
                     hypervisor  and  other nodes, <u>L</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u> <u>N</u><u>e</u><u>t</u><u>w</u><u>o</u><u>r</u><u>k</u> (LN) tables
                     that describe the logical network in terms  of  ``logical
                     datapath  flows,’’  and  <u>B</u><u>i</u><u>n</u><u>d</u><u>i</u><u>n</u><u>g</u> tables that link logical
                     network components’ locations to  the  physical  network.
                     The  hypervisors populate the PN and Port_Binding tables,
                     whereas <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>(8) populates the LN tables.

                     OVN Southbound Database performance must scale  with  the
                     number of transport nodes.  This will likely require some
                     work on  <b>o</b><b>v</b><b>s</b><b>d</b><b>b</b><b>-</b><b>s</b><b>e</b><b>r</b><b>v</b><b>e</b><b>r</b>(1)  as  we  encounter  bottlenecks.
                     Clustering for availability may be needed.

       The remaining components are replicated onto each hypervisor:

              ·      <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>(8)  is  OVN’s agent on each hypervisor and
                     software gateway.  Northbound, it  connects  to  the  OVN
                     Southbound  Database to learn about OVN configuration and
                     status and to populate the PN table and the <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>  col‐
                     umn  in  <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b>  table  with  the  hypervisor’s  status.
                     Southbound, it connects to <b>o</b><b>v</b><b>s</b><b>-</b><b>v</b><b>s</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b><b>d</b>(8) as an OpenFlow
                     controller,  for control over network traffic, and to the
                     local <b>o</b><b>v</b><b>s</b><b>d</b><b>b</b><b>-</b><b>s</b><b>e</b><b>r</b><b>v</b><b>e</b><b>r</b>(1) to allow it to monitor and  control
                     Open vSwitch configuration.

              ·      <b>o</b><b>v</b><b>s</b><b>-</b><b>v</b><b>s</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b><b>d</b>(8) and <b>o</b><b>v</b><b>s</b><b>d</b><b>b</b><b>-</b><b>s</b><b>e</b><b>r</b><b>v</b><b>e</b><b>r</b>(1) are conventional com‐
                     ponents of Open vSwitch.

                                         CMS
                                          |
                                          |
                              +-----------|-----------+
                              |           |           |
                              |     OVN/CMS Plugin    |
                              |           |           |
                              |           |           |
                              |   OVN Northbound DB   |
                              |           |           |
                              |           |           |
                              |       ovn-northd      |
                              |           |           |
                              +-----------|-----------+
                                          |
                                          |
                                +-------------------+
                                | OVN Southbound DB |
                                +-------------------+
                                          |
                                          |
                       +------------------+------------------+
                       |                  |                  |
         HV 1          |                  |    HV n          |
       +---------------|---------------+  .  +---------------|---------------+
       |               |               |  .  |               |               |
       |        ovn-controller         |  .  |        ovn-controller         |
       |         |          |          |  .  |         |          |          |
       |         |          |          |     |         |          |          |
       |  ovs-vswitchd   ovsdb-server  |     |  ovs-vswitchd   ovsdb-server  |
       |                               |     |                               |
       +-------------------------------+     +-------------------------------+

   <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> <b>S</b><b>e</b><b>t</b><b>u</b><b>p</b>
       Each chassis in an OVN deployment  must  be  configured  with  an  Open
       vSwitch  bridge dedicated for OVN’s use, called the <u>i</u><u>n</u><u>t</u><u>e</u><u>g</u><u>r</u><u>a</u><u>t</u><u>i</u><u>o</u><u>n</u> <u>b</u><u>r</u><u>i</u><u>d</u><u>g</u><u>e</u>.
       System startup  scripts  may  create  this  bridge  prior  to  starting
       <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> if desired.  If this bridge does not exist when ovn-con‐
       troller starts, it will be created automatically with the default  con‐
       figuration  suggested  below.   The  ports  on  the  integration bridge
       include:

              ·      On any chassis, tunnel ports that OVN  uses  to  maintain
                     logical   network   connectivity.   <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>  adds,
                     updates, and removes these tunnel ports.

              ·      On a hypervisor, any VIFs that are to be attached to log‐
                     ical networks.  The hypervisor itself, or the integration
                     between Open vSwitch and  the  hypervisor  (described  in
                     <b>I</b><b>n</b><b>t</b><b>e</b><b>g</b><b>r</b><b>a</b><b>t</b><b>i</b><b>o</b><b>n</b><b>G</b><b>u</b><b>i</b><b>d</b><b>e</b><b>.</b><b>m</b><b>d</b>)  takes  care  of this.  (This is not
                     part of OVN or new to OVN; this is pre-existing  integra‐
                     tion  work that has already been done on hypervisors that
                     support OVS.)

              ·      On a gateway, the physical port used for logical  network
                     connectivity.   System  startup  scripts add this port to
                     the bridge prior to starting <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>.  This can be
                     a  patch  port  to  another bridge, instead of a physical
                     port, in more sophisticated setups.

       Other ports should not be attached to the integration bridge.  In  par‐
       ticular, physical ports attached to the underlay network (as opposed to
       gateway ports, which are physical ports attached to  logical  networks)
       must  not  be  attached  to  the integration bridge.  Underlay physical
       ports should instead be attached to  a  separate  Open  vSwitch  bridge
       (they need not be attached to any bridge at all, in fact).

       The  integration  bridge  should be configured as described below.  The
       effect   of   each    of    these    settings    is    documented    in
       <b>o</b><b>v</b><b>s</b><b>-</b><b>v</b><b>s</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b><b>d</b><b>.</b><b>c</b><b>o</b><b>n</b><b>f</b><b>.</b><b>d</b><b>b</b>(5):

              <b>f</b><b>a</b><b>i</b><b>l</b><b>-</b><b>m</b><b>o</b><b>d</b><b>e</b><b>=</b><b>s</b><b>e</b><b>c</b><b>u</b><b>r</b><b>e</b>
                     Avoids  switching  packets  between isolated logical net‐
                     works before <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> starts  up.   See  <b>C</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>
                     <b>F</b><b>a</b><b>i</b><b>l</b><b>u</b><b>r</b><b>e</b> <b>S</b><b>e</b><b>t</b><b>t</b><b>i</b><b>n</b><b>g</b><b>s</b> in <b>o</b><b>v</b><b>s</b><b>-</b><b>v</b><b>s</b><b>c</b><b>t</b><b>l</b>(8) for more information.

              <b>o</b><b>t</b><b>h</b><b>e</b><b>r</b><b>-</b><b>c</b><b>o</b><b>n</b><b>f</b><b>i</b><b>g</b><b>:</b><b>d</b><b>i</b><b>s</b><b>a</b><b>b</b><b>l</b><b>e</b><b>-</b><b>i</b><b>n</b><b>-</b><b>b</b><b>a</b><b>n</b><b>d</b><b>=</b><b>t</b><b>r</b><b>u</b><b>e</b>
                     Suppresses  in-band  control  flows  for  the integration
                     bridge.  It would be unusual for such flows  to  show  up
                     anyway,  because OVN uses a local controller (over a Unix
                     domain socket) instead of a remote controller.  It’s pos‐
                     sible,  however, for some other bridge in the same system
                     to have an in-band remote controller, and  in  that  case
                     this  suppresses  the  flows  that  in-band control would
                     ordinarily set up.  See <b>I</b><b>n</b><b>-</b><b>B</b><b>a</b><b>n</b><b>d</b> <b>C</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b> in <b>D</b><b>E</b><b>S</b><b>I</b><b>G</b><b>N</b><b>.</b><b>m</b><b>d</b>  for
                     more information.

       The  customary  name  for the integration bridge is <b>b</b><b>r</b><b>-</b><b>i</b><b>n</b><b>t</b>, but another
       name may be used.

   <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b> <b>N</b><b>e</b><b>t</b><b>w</b><b>o</b><b>r</b><b>k</b><b>s</b>
       A <u>l</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u> <u>n</u><u>e</u><u>t</u><u>w</u><u>o</u><u>r</u><u>k</u> implements the same concepts  as  physical  networks,
       but  they are insulated from the physical network with tunnels or other
       encapsulations.  This allows logical networks to have separate  IP  and
       other address spaces that overlap, without conflicting, with those used
       for physical networks.  Logical  network  topologies  can  be  arranged
       without  regard  for  the  topologies of the physical networks on which
       they run.

       Logical network concepts in OVN include:

              ·      <u>L</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u>  <u>s</u><u>w</u><u>i</u><u>t</u><u>c</u><u>h</u><u>e</u><u>s</u>,  the  logical  version   of   Ethernet
                     switches.

              ·      <u>L</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u> <u>r</u><u>o</u><u>u</u><u>t</u><u>e</u><u>r</u><u>s</u>, the logical version of IP routers.  Log‐
                     ical switches and routers can be connected into sophisti‐
                     cated topologies.

              ·      <u>L</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u>  <u>d</u><u>a</u><u>t</u><u>a</u><u>p</u><u>a</u><u>t</u><u>h</u><u>s</u> are the logical version of an OpenFlow
                     switch.  Logical switches and  routers  are  both  imple‐
                     mented as logical datapaths.

   <b>L</b><b>i</b><b>f</b><b>e</b> <b>C</b><b>y</b><b>c</b><b>l</b><b>e</b> <b>o</b><b>f</b> <b>a</b> <b>V</b><b>I</b><b>F</b>
       Tables and their schemas presented in isolation are difficult to under‐
       stand.  Here’s an example.

       A VIF on a hypervisor is a virtual network interface attached either to
       a  VM  or a container running directly on that hypervisor (This is dif‐
       ferent from the interface of a container running inside a VM).

       The steps in this example refer often to details of  the  OVN  and  OVN
       Northbound  database  schemas.   Please  see  <b>o</b><b>v</b><b>n</b><b>-</b><b>s</b><b>b</b>(5)  and <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>b</b>(5),
       respectively, for the full story on these databases.

              1.
                A VIF’s life cycle begins when a CMS administrator  creates  a
                new  VIF  using the CMS user interface or API and adds it to a
                switch (one implemented by OVN as a logical switch).  The  CMS
                updates  its  own  configuration.   This  includes associating
                unique, persistent identifier <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u> and Ethernet address  <u>m</u><u>a</u><u>c</u>
                with the VIF.

              2.
                The  CMS plugin updates the OVN Northbound database to include
                the new VIF, by adding a row to the  <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b>  table.   In
                the  new row, <b>n</b><b>a</b><b>m</b><b>e</b> is <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u>, <b>m</b><b>a</b><b>c</b> is <u>m</u><u>a</u><u>c</u>, <b>s</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b> points to the
                OVN logical switch’s Logical_Switch record, and other  columns
                are initialized appropriately.

              3.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>  receives  the  OVN Northbound database update.  In
                turn, it makes the corresponding updates to the OVN Southbound
                database,  by adding rows to the OVN Southbound database <b>L</b><b>o</b><b>g</b><b>i</b><b>‐</b>
                <b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table to reflect the new port, e.g.  add  a  flow  to
                recognize  that packets destined to the new port’s MAC address
                should be delivered to it, and update the flow  that  delivers
                broadcast  and  multicast packets to include the new port.  It
                also creates a record in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table and  populates  all
                its columns except the column that identifies the <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>.

              4.
                On  every hypervisor, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> receives the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b>
                table updates that <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b> made in the previous  step.   As
                long  as  the  VM  that  owns the VIF is powered off, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>‐</b>
                <b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> cannot do much; it cannot,  for  example,  arrange  to
                send  packets  to or receive packets from the VIF, because the
                VIF does not actually exist anywhere.

              5.
                Eventually, a user powers on the VM that owns the VIF.  On the
                hypervisor where the VM is powered on, the integration between
                the  hypervisor  and  Open  vSwitch  (described  in   <b>I</b><b>n</b><b>t</b><b>e</b><b>g</b><b>r</b><b>a</b><b>‐</b>
                <b>t</b><b>i</b><b>o</b><b>n</b><b>G</b><b>u</b><b>i</b><b>d</b><b>e</b><b>.</b><b>m</b><b>d</b>)  adds  the VIF to the OVN integration bridge and
                stores <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u> in <b>e</b><b>x</b><b>t</b><b>e</b><b>r</b><b>n</b><b>a</b><b>l</b><b>-</b><b>i</b><b>d</b><b>s</b>:<b>i</b><b>f</b><b>a</b><b>c</b><b>e</b><b>-</b><b>i</b><b>d</b> to  indicate  that  the
                interface  is  an instantiation of the new VIF.  (None of this
                code is new in OVN; this is pre-existing integration work that
                has already been done on hypervisors that support OVS.)

              6.
                On  the  hypervisor where the VM is powered on, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>
                notices  <b>e</b><b>x</b><b>t</b><b>e</b><b>r</b><b>n</b><b>a</b><b>l</b><b>-</b><b>i</b><b>d</b><b>s</b>:<b>i</b><b>f</b><b>a</b><b>c</b><b>e</b><b>-</b><b>i</b><b>d</b>  in  the  new  Interface.    In
                response, it updates the local hypervisor’s OpenFlow tables so
                that packets to and from the VIF are properly handled.  After‐
                ward, in the OVN Southbound DB, it updates the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table’s
                <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> column for the row that links the  logical  port  from
                <b>e</b><b>x</b><b>t</b><b>e</b><b>r</b><b>n</b><b>a</b><b>l</b><b>-</b><b>i</b><b>d</b><b>s</b>:<b>i</b><b>f</b><b>a</b><b>c</b><b>e</b><b>-</b><b>i</b><b>d</b> to the hypervisor.

              7.
                Some  CMS  systems, including OpenStack, fully start a VM only
                when its networking is ready.   To  support  this,  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>
                notices  the <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> column updated for the row in <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> ta‐
                ble and pushes this upward by updating the <b>u</b><b>p</b>  column  in  the
                OVN  Northbound database’s <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table to indicate that
                the VIF is now up.  The CMS, if it uses this feature, can then
                react by allowing the VM’s execution to proceed.

              8.
                On  every  hypervisor  but  the  one  where  the  VIF resides,
                <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> notices the completely  populated  row  in  the
                <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b>  table.   This  provides  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>  the physical
                location of the logical port, so  each  instance  updates  the
                OpenFlow tables of its switch (based on logical datapath flows
                in the OVN DB <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table) so that packets to and  from
                the VIF can be properly handled via tunnels.

              9.
                Eventually,  a  user  powers off the VM that owns the VIF.  On
                the hypervisor where the  VM  was  powered  off,  the  VIF  is
                deleted from the OVN integration bridge.

              10.
                On the hypervisor where the VM was powered off, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>
                notices that the VIF was deleted.  In response, it removes the
                <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>  column  content  in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table for the logical
                port.

              11.
                On every hypervisor, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> notices the empty  <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>
                column  in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table’s row for the logical port.  This
                means that <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> no longer knows the  physical  loca‐
                tion  of  the logical port, so each instance updates its Open‐
                Flow table to reflect that.

              12.
                Eventually, when the VIF (or  its  entire  VM)  is  no  longer
                needed  by  anyone, an administrator deletes the VIF using the
                CMS user interface or API.  The CMS updates its own configura‐
                tion.

              13.
                The  CMS  plugin removes the VIF from the OVN Northbound data‐
                base, by deleting its row in the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table.

              14.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b> receives the OVN  Northbound  update  and  in  turn
                updates  the  OVN Southbound database accordingly, by removing
                or updating the rows from the OVN  Southbound  database  <b>L</b><b>o</b><b>g</b><b>i</b><b>‐</b>
                <b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table and <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table that were related to the now-
                destroyed VIF.

              15.
                On every hypervisor, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> receives the  <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b>
                table  updates  that  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>  made  in  the previous step.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> updates OpenFlow tables to reflect the  update,
                although  there  may  not  be  much  to  do, since the VIF had
                already become unreachable when it was removed from the  <b>B</b><b>i</b><b>n</b><b>d</b><b>‐</b>
                <b>i</b><b>n</b><b>g</b> table in a previous step.

   <b>L</b><b>i</b><b>f</b><b>e</b> <b>C</b><b>y</b><b>c</b><b>l</b><b>e</b> <b>o</b><b>f</b> <b>a</b> <b>C</b><b>o</b><b>n</b><b>t</b><b>a</b><b>i</b><b>n</b><b>e</b><b>r</b> <b>I</b><b>n</b><b>t</b><b>e</b><b>r</b><b>f</b><b>a</b><b>c</b><b>e</b> <b>I</b><b>n</b><b>s</b><b>i</b><b>d</b><b>e</b> <b>a</b> <b>V</b><b>M</b>
       OVN  provides  virtual  network  abstractions by converting information
       written in OVN_NB  database  to  OpenFlow  flows  in  each  hypervisor.
       Secure virtual networking for multi-tenants can only be provided if OVN
       controller is the only entity that can modify flows  in  Open  vSwitch.
       When  the Open vSwitch integration bridge resides in the hypervisor, it
       is a fair assumption to make that tenant workloads running  inside  VMs
       cannot make any changes to Open vSwitch flows.

       If  the infrastructure provider trusts the applications inside the con‐
       tainers not to break out and modify the Open vSwitch flows,  then  con‐
       tainers can be run in hypervisors.  This is also the case when contain‐
       ers are run inside the VMs and Open  vSwitch  integration  bridge  with
       flows  added  by  OVN  controller resides in the same VM.  For both the
       above cases, the workflow is the same as explained with an  example  in
       the previous section ("Life Cycle of a VIF").

       This  section talks about the life cycle of a container interface (CIF)
       when containers are created in the VMs and the Open vSwitch integration
       bridge  resides  inside  the  hypervisor.  In this case, even if a con‐
       tainer application breaks out, other tenants are not  affected  because
       the  containers  running  inside the VMs cannot modify the flows in the
       Open vSwitch integration bridge.

       When multiple containers are created inside a VM,  there  are  multiple
       CIFs  associated  with them.  The network traffic associated with these
       CIFs need to reach the Open vSwitch integration bridge running  in  the
       hypervisor for OVN to support virtual network abstractions.  OVN should
       also be able to distinguish network traffic coming from different CIFs.
       There are two ways to distinguish network traffic of CIFs.

       One  way  is  to provide one VIF for every CIF (1:1 model).  This means
       that there could be a lot of network devices in the  hypervisor.   This
       would slow down OVS because of all the additional CPU cycles needed for
       the management of all the VIFs.  It would also  mean  that  the  entity
       creating  the containers in a VM should also be able to create the cor‐
       responding VIFs in the hypervisor.

       The second way is to provide a single VIF  for  all  the  CIFs  (1:many
       model).  OVN could then distinguish network traffic coming from differ‐
       ent CIFs via a tag written in every packet.  OVN  uses  this  mechanism
       and uses VLAN as the tagging mechanism.

              1.
                A CIF’s life cycle begins when a container is spawned inside a
                VM by the either the same CMS that created the VM or a  tenant
                that  owns  that  VM  or even a container Orchestration System
                that is different than the CMS that initially created the  VM.
                Whoever the entity is, it will need to know the <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u> that is
                associated with the network interface of the VM through  which
                the  container  interface’s  network traffic is expected to go
                through.  The entity that creates the container interface will
                also need to choose an unused VLAN inside that VM.

              2.
                The  container spawning entity (either directly or through the
                CMS that manages the underlying  infrastructure)  updates  the
                OVN  Northbound  database  to include the new CIF, by adding a
                row to the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table.  In the new row,  <b>n</b><b>a</b><b>m</b><b>e</b>  is  any
                unique identifier, <b>p</b><b>a</b><b>r</b><b>e</b><b>n</b><b>t</b><b>_</b><b>n</b><b>a</b><b>m</b><b>e</b> is the <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u> of the VM through
                which the CIF’s network traffic is expected to go through  and
                the <b>t</b><b>a</b><b>g</b> is the VLAN tag that identifies the network traffic of
                that CIF.

              3.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b> receives the OVN Northbound  database  update.   In
                turn, it makes the corresponding updates to the OVN Southbound
                database, by adding rows to the OVN Southbound database’s <b>L</b><b>o</b><b>g</b><b>‐</b>
                <b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table to reflect the new port and also by creating a
                new row in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table and populating  all  its  columns
                except the column that identifies the <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>.

              4.
                On  every hypervisor, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> subscribes to the changes
                in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table.  When a new row is created by <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>
                that  includes a value in <b>p</b><b>a</b><b>r</b><b>e</b><b>n</b><b>t</b><b>_</b><b>p</b><b>o</b><b>r</b><b>t</b> column of <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table,
                the <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> in the  hypervisor  whose  OVN  integration
                bridge  has that same value in <u>v</u><u>i</u><u>f</u><u>-</u><u>i</u><u>d</u> in <b>e</b><b>x</b><b>t</b><b>e</b><b>r</b><b>n</b><b>a</b><b>l</b><b>-</b><b>i</b><b>d</b><b>s</b>:<b>i</b><b>f</b><b>a</b><b>c</b><b>e</b><b>-</b><b>i</b><b>d</b>
                updates the local hypervisor’s OpenFlow tables so that packets
                to  and from the VIF with the particular VLAN <b>t</b><b>a</b><b>g</b> are properly
                handled.  Afterward it updates the <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> column of the <b>B</b><b>i</b><b>n</b><b>d</b><b>‐</b>
                <b>i</b><b>n</b><b>g</b> to reflect the physical location.

              5.
                One  can only start the application inside the container after
                the underlying network is ready.  To support this,  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>
                notices  the  updated  <b>c</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b>  column  in  <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b>  table and
                updates the <b>u</b><b>p</b> column in the OVN Northbound  database’s  <b>L</b><b>o</b><b>g</b><b>i</b><b>‐</b>
                <b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table to indicate that the CIF is now up.  The entity
                responsible to start the container  application  queries  this
                value and starts the application.

              6.
                Eventually  the entity that created and started the container,
                stops it.  The entity, through the CMS (or  directly)  deletes
                its row in the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table.

              7.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>  receives  the  OVN  Northbound  update and in turn
                updates the OVN Southbound database accordingly,  by  removing
                or  updating  the  rows from the OVN Southbound database <b>L</b><b>o</b><b>g</b><b>i</b><b>‐</b>
                <b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table that were related to the now-destroyed CIF.  It
                also deletes the row in the <b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table for that CIF.

              8.
                On  every hypervisor, <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> receives the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b>
                table updates that  <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>o</b><b>r</b><b>t</b><b>h</b><b>d</b>  made  in  the  previous  step.
                <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> updates OpenFlow tables to reflect the update.

   <b>L</b><b>i</b><b>f</b><b>e</b> <b>C</b><b>y</b><b>c</b><b>l</b><b>e</b> <b>o</b><b>f</b> <b>a</b> <b>P</b><b>a</b><b>c</b><b>k</b><b>e</b><b>t</b>
       This section describes how a packet travels from one virtual machine or
       container to another through OVN.   This  description  focuses  on  the
       physical  treatment  of a packet; for a description of the logical life
       cycle of a packet, please refer to the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table in <b>o</b><b>v</b><b>n</b><b>-</b><b>s</b><b>b</b>(5).

       This section mentions several data and  metadata  fields,  for  clarity
       summarized here:

              tunnel key
                     When  OVN encapsulates a packet in Geneve or another tun‐
                     nel, it attaches extra data to it to allow the  receiving
                     OVN instance to process it correctly.  This takes differ‐
                     ent forms depending on the particular encapsulation,  but
                     in  each  case we refer to it here as the ``tunnel key.’’
                     See <b>T</b><b>u</b><b>n</b><b>n</b><b>e</b><b>l</b> <b>E</b><b>n</b><b>c</b><b>a</b><b>p</b><b>s</b><b>u</b><b>l</b><b>a</b><b>t</b><b>i</b><b>o</b><b>n</b><b>s</b>, below, for details.

              logical datapath field
                     A field that denotes the logical datapath through which a
                     packet is being processed.  OVN uses the field that Open‐
                     Flow 1.1+ simply (and confusingly) calls ``metadata’’  to
                     store the logical datapath.  (This field is passed across
                     tunnels as part of the tunnel key.)

              logical input port field
                     A field that denotes the  logical  port  from  which  the
                     packet  entered the logical datapath.  OVN stores this in
                     Nicira extension  register  number  6.   (This  field  is
                     passed across tunnels as part of the tunnel key.)

              logical output port field
                     A  field  that  denotes  the  logical port from which the
                     packet will leave the logical datapath.  This is initial‐
                     ized  to  0 at the beginning of the logical ingress pipe‐
                     line.  OVN stores this in Nicira extension register  num‐
                     ber  7.   (This field is passed across tunnels as part of
                     the tunnel key.)

              VLAN ID
                     The VLAN ID is used as an interface between OVN and  con‐
                     tainers nested inside a VM (see <b>L</b><b>i</b><b>f</b><b>e</b> <b>C</b><b>y</b><b>c</b><b>l</b><b>e</b> <b>o</b><b>f</b> <b>a</b> <b>c</b><b>o</b><b>n</b><b>t</b><b>a</b><b>i</b><b>n</b><b>e</b><b>r</b>
                     <b>i</b><b>n</b><b>t</b><b>e</b><b>r</b><b>f</b><b>a</b><b>c</b><b>e</b> <b>i</b><b>n</b><b>s</b><b>i</b><b>d</b><b>e</b> <b>a</b> <b>V</b><b>M</b>, above, for more information).

       Initially, a VM or container on the ingress hypervisor sends  a  packet
       on a port attached to the OVN integration bridge.  Then:

              1.
                OpenFlow table 0 performs physical-to-logical translation.  It
                matches the packet’s ingress port.  Its actions  annotate  the
                packet  with logical metadata, by setting the logical datapath
                field to identify the logical  datapath  that  the  packet  is
                traversing  and  the  logical input port field to identify the
                ingress port.  Then it resubmits to table 16 to enter the log‐
                ical ingress pipeline.

                It’s possible that a single ingress physical port maps to mul‐
                tiple logical ports with a type of <b>l</b><b>o</b><b>c</b><b>a</b><b>l</b><b>n</b><b>e</b><b>t</b>. The logical data‐
                path  and  logical  input  port  fields  will be reset and the
                packet will be resubmitted to table 16 multiple times.

                Packets that originate from a container nested within a VM are
                treated  in  a  slightly  different way.  The originating con‐
                tainer can be distinguished based on the VIF-specific VLAN ID,
                so  the  physical-to-logical  translation  flows  additionally
                match on VLAN ID and the actions strip the VLAN header.   Fol‐
                lowing this step, OVN treats packets from containers just like
                any other packets.

                Table 0 also processes packets that arrive from other chassis.
                It  distinguishes  them  from  other  packets by ingress port,
                which is a tunnel.  As with  packets  just  entering  the  OVN
                pipeline,  the  actions  annotate  these  packets with logical
                datapath and logical ingress port metadata.  In addition,  the
                actions  set the logical output port field, which is available
                because in OVN tunneling occurs after the logical output  port
                is known.  These three pieces of information are obtained from
                the tunnel encapsulation metadata (see  <b>T</b><b>u</b><b>n</b><b>n</b><b>e</b><b>l</b>  <b>E</b><b>n</b><b>c</b><b>a</b><b>p</b><b>s</b><b>u</b><b>l</b><b>a</b><b>t</b><b>i</b><b>o</b><b>n</b><b>s</b>
                for  encoding details).  Then the actions resubmit to table 33
                to enter the logical egress pipeline.

              2.
                OpenFlow tables 16 through  31  execute  the  logical  ingress
                pipeline  from  the  <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b>  table in the OVN Southbound
                database.  These tables are expressed  entirely  in  terms  of
                logical  concepts like logical ports and logical datapaths.  A
                big part of <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>’s job is  to  translate  them  into
                equivalent  OpenFlow  (in  particular  it translates the table
                numbers: <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> tables  0  through  15  become  OpenFlow
                tables  16  through  31).   For  a  given  packet, the logical
                ingress pipeline  eventually  executes  zero  or  more  <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b>
                actions:

                ·      If  the pipeline executes no <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b> actions at all, the
                       packet is effectively dropped.

                ·      Most commonly, the pipeline executes one <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b> action,
                       which  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b>  implements  by  resubmitting the
                       packet to table 32.

                ·      If the  pipeline  can  execute  more  than  one  <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b>
                       action,  then each one is separately resubmitted to ta‐
                       ble 32.  This can be used to send  multiple  copies  of
                       the  packet  to multiple ports.  (If the packet was not
                       modified between the <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b> actions, and  some  of  the
                       copies  are destined to the same hypervisor, then using
                       a logical multicast output port  would  save  bandwidth
                       between hypervisors.)

              3.
                OpenFlow  tables  32 through 47 implement the <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b> action in
                the logical ingress pipeline.  Specifically, table 32  handles
                packets to remote hypervisors, table 33 handles packets to the
                local hypervisor, and table 34 discards packets whose  logical
                ingress and egress port are the same.

                Each  flow  in  table  32 matches on a logical output port for
                unicast or multicast logical ports that include a logical port
                on a remote hypervisor.  Each flow’s actions implement sending
                a packet to the port it matches.  For unicast  logical  output
                ports on remote hypervisors, the actions set the tunnel key to
                the correct value, then send the packet on the tunnel port  to
                the  correct hypervisor.  (When the remote hypervisor receives
                the packet, table 0 there will  recognize  it  as  a  tunneled
                packet  and pass it along to table 33.)  For multicast logical
                output ports, the actions send one copy of the packet to  each
                remote  hypervisor,  in  the  same way as for unicast destina‐
                tions.  If a multicast group includes a logical port or  ports
                on the local hypervisor, then its actions also resubmit to ta‐
                ble 33.  Table 32 also includes a fallback flow that resubmits
                to table 33 if there is no other match.

                Flows  in  table 33 resemble those in table 32 but for logical
                ports that reside locally rather than remotely.   For  unicast
                logical output ports on the local hypervisor, the actions just
                resubmit to table 34.  For multicast output ports that include
                one  or  more  logical ports on the local hypervisor, for each
                such logical port <u>P</u>, the actions  change  the  logical  output
                port to <u>P</u>, then resubmit to table 34.

                Table 34 matches and drops packets for which the logical input
                and output ports are the same.  It resubmits other packets  to
                table 48.

              4.
                OpenFlow tables 48 through 63 execute the logical egress pipe‐
                line from the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>F</b><b>l</b><b>o</b><b>w</b> table in the OVN  Southbound  data‐
                base.   The egress pipeline can perform a final stage of vali‐
                dation before packet delivery.  Eventually, it may execute  an
                <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b> action, which <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b> implements by resubmitting
                to table 64.  A packet for which the pipeline  never  executes
                <b>o</b><b>u</b><b>t</b><b>p</b><b>u</b><b>t</b>  is  effectively  dropped  (although  it  may have been
                transmitted through a tunnel across a physical network).

                The egress pipeline cannot change the logical output  port  or
                cause further tunneling.

              5.
                OpenFlow  table  64  performs logical-to-physical translation,
                the opposite of table 0.   It  matches  the  packet’s  logical
                egress  port.   Its  actions  output  the  packet  to the port
                attached to the OVN integration bridge  that  represents  that
                logical  port.   If  the  logical  egress  port is a container
                nested with a VM, then before sending the packet  the  actions
                push on a VLAN header with an appropriate VLAN ID.

   <b>L</b><b>i</b><b>f</b><b>e</b> <b>C</b><b>y</b><b>c</b><b>l</b><b>e</b> <b>o</b><b>f</b> <b>a</b> <b>V</b><b>T</b><b>E</b><b>P</b> <b>g</b><b>a</b><b>t</b><b>e</b><b>w</b><b>a</b><b>y</b>
       A  gateway  is  a chassis that forwards traffic between the OVN-managed
       part of a logical network and a physical  VLAN,   extending  a  tunnel-
       based logical network into a physical network.

       The  steps  below  refer  often to details of the OVN and VTEP database
       schemas.  Please see <b>o</b><b>v</b><b>n</b><b>-</b><b>s</b><b>b</b>(5), <b>o</b><b>v</b><b>n</b><b>-</b><b>n</b><b>b</b>(5)  and  <b>v</b><b>t</b><b>e</b><b>p</b>(5),  respectively,
       for the full story on these databases.

              1.
                A VTEP gateway’s life cycle begins with the administrator reg‐
                istering the VTEP gateway as a <b>P</b><b>h</b><b>y</b><b>s</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>S</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b> table entry  in
                the  <b>V</b><b>T</b><b>E</b><b>P</b> database.  The <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> connected to this
                VTEP database, will recognize the new VTEP gateway and  create
                a  new  <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> table entry for it in the <b>O</b><b>V</b><b>N</b><b>_</b><b>S</b><b>o</b><b>u</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b> data‐
                base.

              2.
                The administrator can then create a new  <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>S</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b>  table
                entry,  and bind a particular vlan on a VTEP gateway’s port to
                any VTEP logical switch.  Once a VTEP logical switch is  bound
                to  a VTEP gateway, the <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> will detect it and
                add its name to the <u>v</u><u>t</u><u>e</u><u>p</u><b>_</b><u>l</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u><b>_</b><u>s</u><u>w</u><u>i</u><u>t</u><u>c</u><u>h</u><u>e</u><u>s</u> column of the  <b>C</b><b>h</b><b>a</b><b>s</b><b>‐</b>
                <b>s</b><b>i</b><b>s</b>  table  in  the  <b>O</b><b>V</b><b>N</b><b>_</b><b>S</b><b>o</b><b>u</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b>  database.  Note, the <u>t</u><u>u</u><u>n</u><u>‐</u>
                <u>n</u><u>e</u><u>l</u><b>_</b><u>k</u><u>e</u><u>y</u> column of VTEP logical switch is not  filled  at  cre‐
                ation.   The  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> will set the column when the
                correponding vtep logical switch is bound to  an  OVN  logical
                network.

              3.
                Now,  the  administrator can use the CMS to add a VTEP logical
                switch to the OVN logical network.  To do that, the  CMS  must
                first  create a new <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>P</b><b>o</b><b>r</b><b>t</b> table entry in the <b>O</b><b>V</b><b>N</b><b>_</b><b>N</b><b>o</b><b>r</b><b>t</b><b>h</b><b>‐</b>
                <b>b</b><b>o</b><b>u</b><b>n</b><b>d</b> database.  Then, the <u>t</u><u>y</u><u>p</u><u>e</u> column of this entry  must  be
                set  to "vtep".  Next, the <u>v</u><u>t</u><u>e</u><u>p</u><u>-</u><u>l</u><u>o</u><u>g</u><u>i</u><u>c</u><u>a</u><u>l</u><u>-</u><u>s</u><u>w</u><u>i</u><u>t</u><u>c</u><u>h</u> and <u>v</u><u>t</u><u>e</u><u>p</u><u>-</u><u>p</u><u>h</u><u>y</u><u>s</u><u>i</u><u>‐</u>
                <u>c</u><u>a</u><u>l</u><u>-</u><u>s</u><u>w</u><u>i</u><u>t</u><u>c</u><u>h</u> keys in the <u>o</u><u>p</u><u>t</u><u>i</u><u>o</u><u>n</u><u>s</u> column must also be  specified,
                since multiple VTEP gateways can attach to the same VTEP logi‐
                cal switch.

              4.
                The newly created logical port in the <b>O</b><b>V</b><b>N</b><b>_</b><b>N</b><b>o</b><b>r</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b>  database
                and  its  configuration  will be passed down to the <b>O</b><b>V</b><b>N</b><b>_</b><b>S</b><b>o</b><b>u</b><b>t</b><b>h</b><b>‐</b>
                <b>b</b><b>o</b><b>u</b><b>n</b><b>d</b>  database  as  a  new  <b>P</b><b>o</b><b>r</b><b>t</b><b>_</b><b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b>  table  entry.   The
                <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b>  will  recognize  the  change and bind the
                logical port to the corresponding VTEP gateway chassis.   Con‐
                figuration  of  binding the same VTEP logical switch to a dif‐
                ferent OVN logical networks is not allowed and a warning  will
                be generated in the log.

              5.
                Beside  binding  to  the  VTEP  gateway  chassis, the <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>‐</b>
                <b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> will update the <u>t</u><u>u</u><u>n</u><u>n</u><u>e</u><u>l</u><b>_</b><u>k</u><u>e</u><u>y</u>  column  of  the  VTEP
                logical  switch  to  the  corresponding <b>D</b><b>a</b><b>t</b><b>a</b><b>p</b><b>a</b><b>t</b><b>h</b><b>_</b><b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table
                entry’s <u>t</u><u>u</u><u>n</u><u>n</u><u>e</u><u>l</u><b>_</b><u>k</u><u>e</u><u>y</u> for the bound OVN logical network.

              6.
                Next, the <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> will keep reacting to  the  con‐
                figuration  change  in  the <b>P</b><b>o</b><b>r</b><b>t</b><b>_</b><b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> in the <b>O</b><b>V</b><b>N</b><b>_</b><b>N</b><b>o</b><b>r</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b>
                database, and updating the <b>U</b><b>c</b><b>a</b><b>s</b><b>t</b><b>_</b><b>M</b><b>a</b><b>c</b><b>s</b><b>_</b><b>R</b><b>e</b><b>m</b><b>o</b><b>t</b><b>e</b> table in the <b>V</b><b>T</b><b>E</b><b>P</b>
                database.  This allows the VTEP gateway to understand where to
                forward the unicast traffic coming from the extended  external
                network.

              7.
                Eventually, the VTEP gateway’s life cycle ends when the admin‐
                istrator unregisters the VTEP gateway from the <b>V</b><b>T</b><b>E</b><b>P</b>  database.
                The  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b>  will  recognize the event and remove
                all related configurations (<b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> table entry and port bind‐
                ings) in the <b>O</b><b>V</b><b>N</b><b>_</b><b>S</b><b>o</b><b>u</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b> database.

              8.
                When  the  <b>o</b><b>v</b><b>n</b><b>-</b><b>c</b><b>o</b><b>n</b><b>t</b><b>r</b><b>o</b><b>l</b><b>l</b><b>e</b><b>r</b><b>-</b><b>v</b><b>t</b><b>e</b><b>p</b> is terminated, all related con‐
                figurations in the <b>O</b><b>V</b><b>N</b><b>_</b><b>S</b><b>o</b><b>u</b><b>t</b><b>h</b><b>b</b><b>o</b><b>u</b><b>n</b><b>d</b> database and the <b>V</b><b>T</b><b>E</b><b>P</b>  data‐
                base  will be cleaned, including <b>C</b><b>h</b><b>a</b><b>s</b><b>s</b><b>i</b><b>s</b> table entries for all
                registered VTEP gateways and  their  port  bindings,  and  all
                <b>U</b><b>c</b><b>a</b><b>s</b><b>t</b><b>_</b><b>M</b><b>a</b><b>c</b><b>s</b><b>_</b><b>R</b><b>e</b><b>m</b><b>o</b><b>t</b><b>e</b>  table entries and the <b>L</b><b>o</b><b>g</b><b>i</b><b>c</b><b>a</b><b>l</b><b>_</b><b>S</b><b>w</b><b>i</b><b>t</b><b>c</b><b>h</b> tunnel
                keys.

<b>D</b><b>E</b><b>S</b><b>I</b><b>G</b><b>N</b> <b>D</b><b>E</b><b>C</b><b>I</b><b>S</b><b>I</b><b>O</b><b>N</b><b>S</b>
   <b>T</b><b>u</b><b>n</b><b>n</b><b>e</b><b>l</b> <b>E</b><b>n</b><b>c</b><b>a</b><b>p</b><b>s</b><b>u</b><b>l</b><b>a</b><b>t</b><b>i</b><b>o</b><b>n</b><b>s</b>
       OVN annotates logical network packets that it sends from one hypervisor
       to  another  with  the  following  three  pieces of metadata, which are
       encoded in an encapsulation-specific fashion:

              ·      24-bit logical datapath identifier, from  the  <b>t</b><b>u</b><b>n</b><b>n</b><b>e</b><b>l</b><b>_</b><b>k</b><b>e</b><b>y</b>
                     column in the OVN Southbound <b>D</b><b>a</b><b>t</b><b>a</b><b>p</b><b>a</b><b>t</b><b>h</b><b>_</b><b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table.

              ·      15-bit logical ingress port identifier.  ID 0 is reserved
                     for internal use within OVN.  IDs 1 through 32767, inclu‐
                     sive,  may  be  assigned  to  logical ports (see the <b>t</b><b>u</b><b>n</b><b>‐</b>
                     <b>n</b><b>e</b><b>l</b><b>_</b><b>k</b><b>e</b><b>y</b> column in the OVN Southbound <b>P</b><b>o</b><b>r</b><b>t</b><b>_</b><b>B</b><b>i</b><b>n</b><b>d</b><b>i</b><b>n</b><b>g</b> table).

              ·      16-bit logical egress port  identifier.   IDs  0  through
                     32767 have the same meaning as for logical ingress ports.
                     IDs 32768 through 65535, inclusive, may  be  assigned  to
                     logical  multicast  groups  (see the <b>t</b><b>u</b><b>n</b><b>n</b><b>e</b><b>l</b><b>_</b><b>k</b><b>e</b><b>y</b> column in
                     the OVN Southbound <b>M</b><b>u</b><b>l</b><b>t</b><b>i</b><b>c</b><b>a</b><b>s</b><b>t</b><b>_</b><b>G</b><b>r</b><b>o</b><b>u</b><b>p</b> table).

       For hypervisor-to-hypervisor traffic, OVN supports only Geneve and  STT
       encapsulations, for the following reasons:

              ·      Only STT and Geneve support the large amounts of metadata
                     (over 32 bits per packet) that  OVN  uses  (as  described
                     above).

              ·      STT  and  Geneve  use  randomized UDP or TCP source ports
                     that allows efficient distribution among  multiple  paths
                     in environments that use ECMP in their underlay.

              ·      NICs  are  available to offload STT and Geneve encapsula‐
                     tion and decapsulation.

       Due to its flexibility, the preferred encapsulation between hypervisors
       is  Geneve.   For Geneve encapsulation, OVN transmits the logical data‐
       path identifier in the Geneve VNI.  OVN transmits the  logical  ingress
       and  logical  egress  ports  in  a TLV with class 0xffff, type 0, and a
       32-bit value encoded as follows, from MSB to LSB:

              ·      1 bits: rsv (0)

              ·      15 bits: ingress port

              ·      16 bits: egress port


       Environments whose NICs lack Geneve offload may prefer  STT  encapsula‐
       tion  for  performance reasons.  For STT encapsulation, OVN encodes all
       three pieces of logical metadata in the STT 64-bit tunnel  ID  as  fol‐
       lows, from MSB to LSB:

              ·      9 bits: reserved (0)

              ·      15 bits: ingress port

              ·      16 bits: egress port

              ·      24 bits: datapath


       For connecting to gateways, in addition to Geneve and STT, OVN supports
       VXLAN, because only  VXLAN  support  is  common  on  top-of-rack  (ToR)
       switches.   Currently,  gateways  have  a  feature set that matches the
       capabilities as defined by the VTEP schema, so fewer bits  of  metadata
       are  necessary.  In the future, gateways that do not support encapsula‐
       tions with large amounts of metadata may continue  to  have  a  reduced
       feature set.



Open vSwitch 2.4.90            OVN Architecture            ovn-architecture(7)
</pre></body></html>
